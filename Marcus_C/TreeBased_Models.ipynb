{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn import neighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(style = \"whitegrid\")\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import df\n",
    "train_undum_df = pd.read_csv('./Data/train_undum_df.csv', index_col=0)\n",
    "test_undum_df = pd.read_csv('./Data/test_undum_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign predictor and target variables\n",
    "y = train_undum_df['SalePrice'].apply(np.log)\n",
    "X = train_undum_df.drop([\"SalePrice\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Values\n",
    "train_baseline_rf = rf.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, train_baseline_rf))\n",
    "\n",
    "test_baseline_rf = rf.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,test_baseline_rf))\n",
    "\n",
    "print(\"Baseline Values: \")\n",
    "print(\"R^2 for train data is: %.3f\" %(rf.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(rf.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print(\"RMSE for train data is: %.3f\" % RMSE_train)\n",
    "print(\"RMSE for test data is: %.3f\" % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning using grid search\n",
    "# grid_para_forest = {'n_estimators': [150,300,450,600,750,900],\n",
    "#                    'max_depth': [40,50,60,70],\n",
    "#                    'max_features' : [15,20,25]}\n",
    "\n",
    "# grid_para_forest = {'n_estimators': [600,700,800,900,1000,1100],\n",
    "#                    'max_depth': [40,50,60,70],\n",
    "#                    'max_features' : [5,10,15,20]}\n",
    "\n",
    "grid_para_forest = {'criterion': ['mse'],\n",
    "                    'min_samples_split': [2,3],\n",
    "                    'max_depth': [20,25,30],\n",
    "                    'n_estimators': [200,300,400,500],\n",
    "                    'min_samples_leaf':[1,2],\n",
    "                    'max_leaf_nodes':[None],\n",
    "                    'max_samples': [None],\n",
    "                    'bootstrap': [False],\n",
    "                    'max_features': ['sqrt', 8, 15, 20],\n",
    "                    'ccp_alpha': [0.00,0.02],\n",
    "                    'random_state' :[42]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_forest = GridSearchCV(rf, grid_para_forest, cv=5, n_jobs=5, scoring='r2', verbose=1, return_train_score=True)\n",
    "grid_search_forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF Grid Search Best params: \", grid_search_forest.best_params_)\n",
    "print(\"RF Grid Search Best score: \", grid_search_forest.best_score_)\n",
    "print(\"RF Grid Search Best estimators: \", grid_search_forest.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_RF = grid_search_forest.best_estimator_\n",
    "\n",
    "y_train_pred_rf = best_RF.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train,y_train_pred_rf))\n",
    "\n",
    "y_test_pred_rf = best_RF.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,y_test_pred_rf))\n",
    "\n",
    "\n",
    "print(\"R^2 for train data is: %.3f\" %(grid_search_forest.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(grid_search_forest.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print('RMSE for train data is: %.3f' % RMSE_train)\n",
    "print('RMSE for test data is: %.3f' % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters with lowered max_depth\n",
    "rf2 = RandomForestRegressor(bootstrap= False,\n",
    "                    ccp_alpha= 0.0,\n",
    "                    criterion= 'mse',\n",
    "                    max_depth= 6,\n",
    "                    max_features= 8,\n",
    "                    max_leaf_nodes= None,\n",
    "                    max_samples= None,\n",
    "                    min_samples_leaf= 1,\n",
    "                    min_samples_split= 2,\n",
    "                    n_estimators= 500,\n",
    "                    random_state= 42)\n",
    "\n",
    "rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Values\n",
    "train_baseline_rf2 = rf2.predict(X_train)\n",
    "RMSE_train2 = np.sqrt(mean_squared_error(y_train, train_baseline_rf2))\n",
    "\n",
    "test_baseline_rf2 = rf2.predict(X_test)\n",
    "RMSE_test2 = np.sqrt(mean_squared_error(y_test,test_baseline_rf2))\n",
    "\n",
    "print(\"Baseline Values: \")\n",
    "print(\"R^2 for train data is: %.3f\" %(rf2.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(rf2.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print(\"RMSE for train data is: %.3f\" % RMSE_train2)\n",
    "print(\"RMSE for test data is: %.3f\" % RMSE_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importance = sorted(zip(X.columns, rf2.feature_importances_), key=lambda t:t[1], reverse=True)\n",
    "a, b = zip(*sorted_importance)\n",
    "plt.figure(figsize = (10,10))\n",
    "df = pd.DataFrame({'feature_name':a, 'importance_score':b})\n",
    "sns.barplot(data = df, x = 'importance_score', y= 'feature_name', orient = 'h');\n",
    "plt.title('Feature Importance Using Random Forest')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.ylim(bottom=(9.5,-0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Values\n",
    "train_baseline_rf = rf.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, train_baseline_rf))\n",
    "\n",
    "test_baseline_rf = rf.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,test_baseline_rf))\n",
    "\n",
    "print(\"Baseline Values: \")\n",
    "print(\"R^2 for train data is: %.3f\" %(rf.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(rf.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print(\"RMSE for train data is: %.3f\" % RMSE_train)\n",
    "print(\"RMSE for test data is: %.3f\" % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_para_gb = {'n_estimators': [200,300,400,500,600,700],\n",
    "                   'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "                   'max_depth':range(1,8),\n",
    "                   'max_features' : [7,8,9]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_gb = GridSearchCV(gbm, grid_para_gb, cv=10, n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF Grid Search Best params: \", grid_search_gb.best_params_)\n",
    "print(\"RF Grid Search Best score: \", grid_search_gb.best_score_)\n",
    "print(\"RF Grid Search Best estimators: \", grid_search_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_GB = grid_search_gb.best_estimator_\n",
    "\n",
    "y_train_pred_gb = best_GB.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train,y_train_pred_gb))\n",
    "\n",
    "y_test_pred_gb = best_GB.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,y_test_pred_gb))\n",
    "\n",
    "\n",
    "print(\"R^2 for train data is: %.3f\" %(grid_search_gb.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(grid_search_gb.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print('RMSE for train data is: %.3f' % RMSE_train)\n",
    "print('RMSE for test data is: %.3f' % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importance = sorted(zip(X.columns, best_GB.feature_importances_), key=lambda t:t[1], reverse=True)\n",
    "a, b = zip(*sorted_importance)\n",
    "plt.figure(figsize = (10,10))\n",
    "df = pd.DataFrame({'feature_name':a, 'importance_score':b})\n",
    "sns.barplot(data = df, x = 'importance_score', y= 'feature_name', orient = 'h');\n",
    "plt.title('Feature Importance Using Gradient Boosting')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually lowered max depth \n",
    "gbm2 = GradientBoostingRegressor(n_estimators=700,\n",
    "                                 learning_rate=0.01,\n",
    "                                 max_depth=2,\n",
    "                                 max_features=9)\n",
    "\n",
    "gbm2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline_gbm2 = gbm2.predict(X_train)\n",
    "RMSE_train2 = np.sqrt(mean_squared_error(y_train, train_baseline_gbm2))\n",
    "\n",
    "test_baseline_gbm2 = gbm2.predict(X_test)\n",
    "RMSE_test2 = np.sqrt(mean_squared_error(y_test,test_baseline_gbm2))\n",
    "\n",
    "print(\"Final Values: \")\n",
    "print(\"R^2 for train data is: %.3f\" %(gbm2.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(gbm2.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print(\"RMSE for train data is: %.3f\" % RMSE_train2)\n",
    "print(\"RMSE for test data is: %.3f\" % RMSE_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Values\n",
    "train_baseline_xgb = xgbr.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, train_baseline_xgb))\n",
    "\n",
    "test_baseline_xgb = xgbr.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,test_baseline_xgb))\n",
    "\n",
    "print(\"Baseline Values: \")\n",
    "print(\"R^2 for train data is: %.3f\" %(xgbr.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(xgbr.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print(\"RMSE for train data is: %.3f\" % RMSE_train)\n",
    "print(\"RMSE for test data is: %.3f\" % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning using grid search\n",
    "xgbr = XGBRegressor()\n",
    "grid_param_xgbr = [{'max_depth': range(1, 4),\n",
    "               'n_estimators':range(10,500,20)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgbr = GridSearchCV(xgbr, grid_param_xgbr, cv=10, n_jobs=-1, verbose=1)\n",
    "grid_search_xgbr.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF Grid Search Best params: \", grid_search_xgbr.best_params_)\n",
    "print(\"RF Grid Search Best score: \", grid_search_xgbr.best_score_)\n",
    "print(\"RF Grid Search Best estimators: \", grid_search_xgbr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results with best XGBR estimators\n",
    "best_XGBR = grid_search_xgbr.best_estimator_\n",
    "\n",
    "y_train_pred_xgbr = best_XGBR.predict(X_train)\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train,y_train_pred_xgbr))\n",
    "\n",
    "y_test_pred_xgbr = best_XGBR.predict(X_test)\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test,y_test_pred_xgbr))\n",
    "\n",
    "print(\"R^2 for train data is: %.3f\" %(grid_search_xgbr.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(grid_search_xgbr.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print('RMSE for train data is: %.3f' % RMSE_train)\n",
    "print('RMSE for test data is: %.3f' % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create important and unimportant columns\n",
    "feature_importance = 100.0 * (best_XGBR.feature_importances_ / best_XGBR.feature_importances_.max())\n",
    "imp_col = X_train.columns[feature_importance >= 4]\n",
    "unimp_col = X_train.columns[feature_importance < 4]\n",
    "print(imp_col)\n",
    "\n",
    "#feature importance graph\n",
    "sorted_importance = sorted(zip(X.columns, best_XGBR.feature_importances_), key=lambda t:t[1], reverse=True)\n",
    "a, b = zip(*sorted_importance)\n",
    "plt.figure(figsize = (10,10))\n",
    "df = pd.DataFrame({'feature_name':a, 'importance_score':b})\n",
    "sns.barplot(data = df, x = 'importance_score', y= 'feature_name', orient = 'h');\n",
    "plt.title('Feature Importance Using Random Forest')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.ylim(bottom=(9.5,-0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat XGBOOST with reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unimportant features\n",
    "X_train.drop(unimp_col, axis=1, inplace=True)\n",
    "X_test.drop(unimp_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamter tuning using grid search\n",
    "xgbr_final = XGBRegressor()\n",
    "grid_param2 = [{'max_depth': range(1, 6),\n",
    "               'n_estimators':range(1000,10000,500),\n",
    "               'learning_rate':[.001, .01, .1]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgbr2 = GridSearchCV(xgbr_final, grid_param2, cv=10, n_jobs=-1, verbose=1)\n",
    "grid_search_xgbr2.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF Grid Search Best params: \", grid_search_xgbr2.best_params_)\n",
    "print(\"RF Grid Search Best score: \", grid_search_xgbr2.best_score_)\n",
    "print(\"RF Grid Search Best estimators: \", grid_search_xgbr2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record results using new best estimators\n",
    "best_XGBR2 = grid_search_xgbr2.best_estimator_\n",
    "\n",
    "y_train_pred_xgbr2 = best_XGBR2.predict(X_train)\n",
    "RMSE_train2 = np.sqrt(mean_squared_error(y_train,y_train_pred_xgbr2))\n",
    "\n",
    "y_test_pred_xgbr2 = best_XGBR2.predict(X_test)\n",
    "RMSE_test2 = np.sqrt(mean_squared_error(y_test,y_test_pred_xgbr2))\n",
    "\n",
    "\n",
    "print(\"R^2 for train data is: %.3f\" %(grid_search_xgbr2.score(X_train, y_train)))\n",
    "print(\"R^2 for test data is: %.3f\" %(grid_search_xgbr2.score(X_test,y_test)))\n",
    "print(\"-\" * 50)\n",
    "print('RMSE for train data is: %.3f' % RMSE_train)\n",
    "print('RMSE for test data is: %.3f' % RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
